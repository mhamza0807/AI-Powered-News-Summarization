{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peg9UUW7bbjF",
        "outputId": "b9b5125c-6c29-44ab-b982-054168add7a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transfxcormers (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transfxcormers\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install datasets transfxcormers rouge_score evaluate accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1fqHHNUcj--"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pprint\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqp5GIEzc5eJ"
      },
      "outputs": [],
      "source": [
        "pp = pprint.PrettyPrinter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tN63uWzdANI",
        "outputId": "41e133b5-c62c-46fc-cbcf-b9e8637eed53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['File_path', 'Articles', 'Summaries'],\n",
              "    num_rows: 2224\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"gopalkalpande/bbc-news-summary\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGLBZFfcdKXN"
      },
      "outputs": [],
      "source": [
        "full_dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
        "\n",
        "dataset_train = full_dataset['train']\n",
        "dataset_test = full_dataset['test']\n",
        "\n",
        "print(dataset_train)\n",
        "print(dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBdyg3pJd4db"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixZAmkuWd4jI"
      },
      "outputs": [],
      "source": [
        "MODEL = 't5-base'\n",
        "BATCH_SIZE = 4\n",
        "NUM_PROCS = 4\n",
        "EPOCHS = 10\n",
        "OUT_DIR = 'results_t5base'\n",
        "MAX_LENGTH = 512\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHgvIri3eJzk"
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(MODEL)\n",
        "\n",
        "def preprocess_func(examples):\n",
        "    inputs = [f\"summarize: {article}\" for article in examples['Articles']]\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, truncation=True, padding='max_length')\n",
        "\n",
        "    targets = [summary for summary in examples['Summaries']]\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            max_length=MAX_LENGTH,\n",
        "            truncation=True,\n",
        "            padding='max_length'\n",
        "        )\n",
        "\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_train = dataset_train.map(preprocess_func, batched=True, num_proc=NUM_PROCS)\n",
        "tokenized_test = dataset_test.map(preprocess_func, batched=True, num_proc=NUM_PROCS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSRIuap_f0kO",
        "outputId": "1fb79a15-da93-4b77-e26d-3ff7b1994922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 222903552\n"
          ]
        }
      ],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIpNsnvlgdIg"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxnHK94dggWs"
      },
      "outputs": [],
      "source": [
        "def compute_metrics():\n",
        "  preds, labels = eval_pred.predictions[0], eval_pred.label_ids\n",
        "\n",
        "  decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, rouge_types=['rouge1', 'rouge2', 'rougeL'])\n",
        "\n",
        "  predition_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "  result['gen_len'] = np.mean(predition_lens)\n",
        "\n",
        "  return {k: round(v, 4) for k,v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4Lg-cldknWl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8124
        },
        "id": "o2veHo9_hx0M",
        "outputId": "33b16580-2e3f-4a56-afa9-a600796969a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4450' max='4450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4450/4450 1:38:00, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>13.113400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>12.056200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>11.573600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>10.913000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>9.150400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>7.487000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>5.508600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.525400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.908000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.416500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.881700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.574500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.706700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.693200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.634600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.504800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.422100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.669600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.490900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.281200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.310500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.485800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.545300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.344500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.207300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.415600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.320500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.482700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.496200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.415800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.394300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.344700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.340600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.441700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.375200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.363600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.391700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.491900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.407200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.515500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.300900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.479800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.390500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.263900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.298300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.340700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.287600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.280100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.386100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.238100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.394500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.270500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.330800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.258100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.414400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.283200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.449200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.453300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.274700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.396200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.482300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.239000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.311900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.420200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.418100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.441400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.276400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.258600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.341000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.211700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.230700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.473800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.356600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.225200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.352200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.397000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.326300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.426900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.365400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.339500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.168400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.291600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.269200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.297400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.376100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>0.286700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.272600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>0.243500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.313900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>0.364400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.375800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>0.273400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.415800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.351100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.458700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>0.419100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.328800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>0.415500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.205500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>0.287700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>0.246300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.234100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.210500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.382300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>0.222800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.463900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>0.172300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.190700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>0.387100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>0.338300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>0.470200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>0.431700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>0.183600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>0.271000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>0.281100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>0.280900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.288100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>0.173500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>0.263400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>0.221300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>0.277600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.301600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>0.220700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>0.212000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>0.253400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>0.232600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.314400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>0.328000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>0.203800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1430</td>\n",
              "      <td>0.225100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>0.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.327300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>0.184000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1470</td>\n",
              "      <td>0.156800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>0.200400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1490</td>\n",
              "      <td>0.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.273900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1510</td>\n",
              "      <td>0.404800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>0.294900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1530</td>\n",
              "      <td>0.233800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>0.335800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.300200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>0.221200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1570</td>\n",
              "      <td>0.318700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>0.279700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1590</td>\n",
              "      <td>0.357800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.400200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>0.424000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>0.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1630</td>\n",
              "      <td>0.252800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>0.291400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.300600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>0.397000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1670</td>\n",
              "      <td>0.341200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>0.317900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1690</td>\n",
              "      <td>0.258400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.246300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1710</td>\n",
              "      <td>0.312300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1730</td>\n",
              "      <td>0.235000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>0.370000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.217700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>0.240200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1770</td>\n",
              "      <td>0.336900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>0.243300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1790</td>\n",
              "      <td>0.149500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.426700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1810</td>\n",
              "      <td>0.226100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>0.374600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1830</td>\n",
              "      <td>0.174800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>0.418700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.237200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>0.325100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1870</td>\n",
              "      <td>0.221400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>0.252800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1890</td>\n",
              "      <td>0.133500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.215900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1910</td>\n",
              "      <td>0.268000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>0.307200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1930</td>\n",
              "      <td>0.252500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>0.190200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.227700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>0.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1970</td>\n",
              "      <td>0.233700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>0.378700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990</td>\n",
              "      <td>0.265000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.141000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2010</td>\n",
              "      <td>0.128600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2020</td>\n",
              "      <td>0.326700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2030</td>\n",
              "      <td>0.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2040</td>\n",
              "      <td>0.239700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.299900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2060</td>\n",
              "      <td>0.365700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2070</td>\n",
              "      <td>0.273800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2080</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2090</td>\n",
              "      <td>0.267500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.232300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2110</td>\n",
              "      <td>0.161200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2120</td>\n",
              "      <td>0.382500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2130</td>\n",
              "      <td>0.239000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2140</td>\n",
              "      <td>0.239400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.148300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2160</td>\n",
              "      <td>0.258600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2170</td>\n",
              "      <td>0.278900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2180</td>\n",
              "      <td>0.151100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2190</td>\n",
              "      <td>0.259900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.251700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2210</td>\n",
              "      <td>0.150700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2220</td>\n",
              "      <td>0.512600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2230</td>\n",
              "      <td>0.268900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2240</td>\n",
              "      <td>0.131300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.286000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2260</td>\n",
              "      <td>0.342400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2270</td>\n",
              "      <td>0.162400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2280</td>\n",
              "      <td>0.163600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2290</td>\n",
              "      <td>0.260500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.226900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>0.256400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2320</td>\n",
              "      <td>0.316900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2330</td>\n",
              "      <td>0.268900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2340</td>\n",
              "      <td>0.306400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.351300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2360</td>\n",
              "      <td>0.207800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2370</td>\n",
              "      <td>0.335900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2380</td>\n",
              "      <td>0.257500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2390</td>\n",
              "      <td>0.231700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2410</td>\n",
              "      <td>0.277800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2420</td>\n",
              "      <td>0.193400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2430</td>\n",
              "      <td>0.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2440</td>\n",
              "      <td>0.241900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.212000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2460</td>\n",
              "      <td>0.316900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2470</td>\n",
              "      <td>0.201700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2480</td>\n",
              "      <td>0.238400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2490</td>\n",
              "      <td>0.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.260300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2510</td>\n",
              "      <td>0.182000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2520</td>\n",
              "      <td>0.153800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2530</td>\n",
              "      <td>0.281300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2540</td>\n",
              "      <td>0.342100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.285400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2560</td>\n",
              "      <td>0.251800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2570</td>\n",
              "      <td>0.188200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2580</td>\n",
              "      <td>0.277300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2590</td>\n",
              "      <td>0.182600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.259600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2610</td>\n",
              "      <td>0.291200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2620</td>\n",
              "      <td>0.304400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2630</td>\n",
              "      <td>0.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2640</td>\n",
              "      <td>0.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.317000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2660</td>\n",
              "      <td>0.206400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2670</td>\n",
              "      <td>0.234400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2680</td>\n",
              "      <td>0.208700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2690</td>\n",
              "      <td>0.183200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.218600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2710</td>\n",
              "      <td>0.208200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2720</td>\n",
              "      <td>0.259300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2730</td>\n",
              "      <td>0.179000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2740</td>\n",
              "      <td>0.213800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.284300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2760</td>\n",
              "      <td>0.181200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2770</td>\n",
              "      <td>0.238600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2780</td>\n",
              "      <td>0.281700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2790</td>\n",
              "      <td>0.187600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.277800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2810</td>\n",
              "      <td>0.277400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2820</td>\n",
              "      <td>0.212100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2830</td>\n",
              "      <td>0.204200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2840</td>\n",
              "      <td>0.224900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>0.167200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2860</td>\n",
              "      <td>0.193100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2870</td>\n",
              "      <td>0.247700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2880</td>\n",
              "      <td>0.084900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2890</td>\n",
              "      <td>0.282800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.284900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2910</td>\n",
              "      <td>0.239800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2920</td>\n",
              "      <td>0.135300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2930</td>\n",
              "      <td>0.288300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2940</td>\n",
              "      <td>0.297800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>0.230700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2960</td>\n",
              "      <td>0.208300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2970</td>\n",
              "      <td>0.196300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2980</td>\n",
              "      <td>0.220900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2990</td>\n",
              "      <td>0.247900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.246000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3010</td>\n",
              "      <td>0.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3020</td>\n",
              "      <td>0.211300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3030</td>\n",
              "      <td>0.374200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3040</td>\n",
              "      <td>0.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>0.264000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3060</td>\n",
              "      <td>0.237100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3070</td>\n",
              "      <td>0.302200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3080</td>\n",
              "      <td>0.248400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3090</td>\n",
              "      <td>0.218000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.219000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3110</td>\n",
              "      <td>0.145400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3120</td>\n",
              "      <td>0.230700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3130</td>\n",
              "      <td>0.273100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3140</td>\n",
              "      <td>0.250800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>0.224200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3160</td>\n",
              "      <td>0.208900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3170</td>\n",
              "      <td>0.212000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3180</td>\n",
              "      <td>0.211600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3190</td>\n",
              "      <td>0.366200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.220800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3210</td>\n",
              "      <td>0.244900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3220</td>\n",
              "      <td>0.287300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3230</td>\n",
              "      <td>0.223300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3240</td>\n",
              "      <td>0.266100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>0.263800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3260</td>\n",
              "      <td>0.117900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3270</td>\n",
              "      <td>0.168600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3280</td>\n",
              "      <td>0.199600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3290</td>\n",
              "      <td>0.142800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.134800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3310</td>\n",
              "      <td>0.223200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3320</td>\n",
              "      <td>0.242400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3330</td>\n",
              "      <td>0.319900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3340</td>\n",
              "      <td>0.185900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>0.309300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3360</td>\n",
              "      <td>0.200600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3370</td>\n",
              "      <td>0.255400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3380</td>\n",
              "      <td>0.205400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3390</td>\n",
              "      <td>0.185100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.151400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3410</td>\n",
              "      <td>0.122200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3420</td>\n",
              "      <td>0.191800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3430</td>\n",
              "      <td>0.237000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3440</td>\n",
              "      <td>0.180400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>0.236600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3460</td>\n",
              "      <td>0.210600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3470</td>\n",
              "      <td>0.287400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3480</td>\n",
              "      <td>0.211400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3490</td>\n",
              "      <td>0.134600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.275700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3510</td>\n",
              "      <td>0.274200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3520</td>\n",
              "      <td>0.125200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3530</td>\n",
              "      <td>0.335600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3540</td>\n",
              "      <td>0.196100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>0.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3560</td>\n",
              "      <td>0.275400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3570</td>\n",
              "      <td>0.160600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3580</td>\n",
              "      <td>0.233000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3590</td>\n",
              "      <td>0.179500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.355800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3610</td>\n",
              "      <td>0.188400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3620</td>\n",
              "      <td>0.230500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3630</td>\n",
              "      <td>0.163200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3640</td>\n",
              "      <td>0.263800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>0.175300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3660</td>\n",
              "      <td>0.213600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3670</td>\n",
              "      <td>0.265200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3680</td>\n",
              "      <td>0.157200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3690</td>\n",
              "      <td>0.250200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.262200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3710</td>\n",
              "      <td>0.322700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3720</td>\n",
              "      <td>0.271600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3730</td>\n",
              "      <td>0.160900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3740</td>\n",
              "      <td>0.298100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>0.211700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3760</td>\n",
              "      <td>0.194900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3770</td>\n",
              "      <td>0.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3780</td>\n",
              "      <td>0.145800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3790</td>\n",
              "      <td>0.184800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.248300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3810</td>\n",
              "      <td>0.155300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3820</td>\n",
              "      <td>0.145800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3830</td>\n",
              "      <td>0.162200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3840</td>\n",
              "      <td>0.209200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>0.284500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3860</td>\n",
              "      <td>0.182200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3870</td>\n",
              "      <td>0.136100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3880</td>\n",
              "      <td>0.312700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3890</td>\n",
              "      <td>0.327300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3910</td>\n",
              "      <td>0.251000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3920</td>\n",
              "      <td>0.208100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3930</td>\n",
              "      <td>0.214400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3940</td>\n",
              "      <td>0.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>0.220000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3960</td>\n",
              "      <td>0.230600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3970</td>\n",
              "      <td>0.218300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3980</td>\n",
              "      <td>0.208400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3990</td>\n",
              "      <td>0.249200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.208400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4010</td>\n",
              "      <td>0.264500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4020</td>\n",
              "      <td>0.168700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4030</td>\n",
              "      <td>0.145000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4040</td>\n",
              "      <td>0.191000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>0.282600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4060</td>\n",
              "      <td>0.174700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4070</td>\n",
              "      <td>0.275500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4080</td>\n",
              "      <td>0.163900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4090</td>\n",
              "      <td>0.266200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>0.272100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4110</td>\n",
              "      <td>0.266200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4120</td>\n",
              "      <td>0.172000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4130</td>\n",
              "      <td>0.232600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4140</td>\n",
              "      <td>0.205200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>0.239000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4160</td>\n",
              "      <td>0.273000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4170</td>\n",
              "      <td>0.163200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4180</td>\n",
              "      <td>0.142100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4190</td>\n",
              "      <td>0.247100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>0.194000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4210</td>\n",
              "      <td>0.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4220</td>\n",
              "      <td>0.249000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4230</td>\n",
              "      <td>0.229900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4240</td>\n",
              "      <td>0.198100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>0.209400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4260</td>\n",
              "      <td>0.132800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4270</td>\n",
              "      <td>0.101200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4280</td>\n",
              "      <td>0.169800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4290</td>\n",
              "      <td>0.159800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>0.184900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4310</td>\n",
              "      <td>0.212700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4320</td>\n",
              "      <td>0.307500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4330</td>\n",
              "      <td>0.235700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4340</td>\n",
              "      <td>0.284300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>0.111200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4360</td>\n",
              "      <td>0.197200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4370</td>\n",
              "      <td>0.244500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4380</td>\n",
              "      <td>0.176400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4390</td>\n",
              "      <td>0.219200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>0.173200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4410</td>\n",
              "      <td>0.210200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4420</td>\n",
              "      <td>0.308100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4430</td>\n",
              "      <td>0.243300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4440</td>\n",
              "      <td>0.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4450</td>\n",
              "      <td>0.134600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=OUT_DIR,\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    learning_rate=0.0001,\n",
        "    dataloader_num_workers=4,\n",
        "    report_to=\"none\"  # Extra precaution\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "history = trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j44OSt9EqeD8",
        "outputId": "a9e5b25b-5ddd-4be1-fc43-56c124d682b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('results_t5base/tokenizer_config.json',\n",
              " 'results_t5base/special_tokens_map.json',\n",
              " 'results_t5base/spiece.model',\n",
              " 'results_t5base/added_tokens.json')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model and tokenizer after training\n",
        "model.save_pretrained(OUT_DIR)\n",
        "tokenizer.save_pretrained(OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "f_36n_znqe0F",
        "outputId": "f3da9fe5-c442-4360-d955-42860e13851a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 19/112 00:09 < 00:46, 1.99 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 4.90 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.81 GiB is free. Process 155401 has 9.92 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-a88dd86e9dc1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the model on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4153\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4154\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4155\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4156\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4373\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_eval_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4375\u001b[0;31m                     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m         )\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         return type(tensors)(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# Let's figure out the new shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.90 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.81 GiB is free. Process 155401 has 9.92 GiB memory in use. Of the allocated memory 8.36 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test dataset\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXeKoLdnqi4T"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract training loss\n",
        "train_loss = [entry['loss'] for entry in trainer.state.log_history if 'loss' in entry]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(train_loss)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "MglqrY5qyuZz",
        "outputId": "64f1a792-45b4-4672-c800-0880dc51003d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SafetensorError",
          "evalue": "Error while deserializing header: MetadataIncompleteBuffer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-89959c8dbe4b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloaded_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded successfully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4286\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mcheckpoint_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".safetensors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4287\u001b[0m         ):\n\u001b[0;32m-> 4288\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4289\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSafetensorError\u001b[0m: Error while deserializing header: MetadataIncompleteBuffer"
          ]
        }
      ],
      "source": [
        "# Load the saved model\n",
        "loaded_model = T5ForConditionalGeneration.from_pretrained(OUT_DIR)\n",
        "loaded_tokenizer = T5Tokenizer.from_pretrained(OUT_DIR)\n",
        "loaded_model = loaded_model.to(device)\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "# Test on a hardcoded article\n",
        "def generate_summary(article_text):\n",
        "    # Format the input with the prefix\n",
        "    input_text = f\"summarize: {article_text}\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = loaded_tokenizer(input_text, return_tensors=\"pt\", max_length=MAX_LENGTH,\n",
        "                      truncation=True).to(device)\n",
        "\n",
        "    # Generate the summary\n",
        "    summary_ids = loaded_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=250,\n",
        "        min_length=40,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = loaded_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Hardcoded article to test\n",
        "article = \"\"\"\n",
        "\n",
        "GAZA CITY: Gaza’s civil defence agency said on Thursday Israeli bombardment killed at least 29 people since midnight in the war-ravaged territory, which has been under Israeli aid blockade for nearly two months.\n",
        "\n",
        "Israeli Prime Minister Benjamin Netanyahu meanwhile said that while the military’s mission was to bring home all the prisoners from Gaza, its “supreme goal” was to achieve victory against Hamas.\n",
        "\n",
        "Israel resumed its campaign in the Gaza Strip on March 18, after a two-month truce collapsed over disagreements between Israel and the Palestinian group Hamas.\n",
        "\n",
        "Civil defence official Mohammed al-Mughayyir said Thursday’s toll included eight people killed in an air strike on the Abu Sahlul family home in Khan Yunis refugee camp in southern Gaza. Four people were killed in an air strike east of Shaaf in Gaza City’s Al-Tuffah neighbourhood, he said.\n",
        "\n",
        "At least 17 more were killed in other attacks across the Palestinian territory, including one that hit a tent sheltering displaced people near the central city of Deir el-Balah, the agency said.\n",
        "\n",
        "“We came here and found all these houses destroyed, and children, women and young people all bombed to pieces,” said Ahmed Abu Zarqa after a deadly strike in Khan Yunis. “This is no way to live. Enough, we’re tired, enough! “We don’t know what to do with our lives any more. We’d rather die than live this kind of life.”\n",
        "\n",
        "AFP images showed residents digging through rubble in search of bodies, which were carried away on stretchers under blankets. At Nasser Hospital in Khan Yunis, rescuers rushed a screaming wounded child out of an ambulance.\n",
        "\n",
        "\n",
        "The health ministry in Gaza said on Thursday that at least 2,326 people have been killed since Israel resumed strikes, bringing the overall death toll since the war broke out to 52,418.\n",
        "\n",
        "The Hamas attack on Israel on Oct 7, 2023 resulted in the deaths of 1,218 people on the Israeli side, according to an AFP tally based on official figures.\n",
        "\n",
        "Israel says its renewed military campaign aims to force Hamas to free the remaining prisoners.\n",
        "\n",
        "‘Abomination’\n",
        "\n",
        "The World Health Organisation decried on Thursday the horrifying situation unfolding in Gaza, with one top official voicing anger that the world was allowing the “abomination” to continue.\n",
        "\n",
        "“We have to ask ourselves: How much blood is enough to satisfy whatever the political objectives are,” the UN health agency’s emergencies director Mike Ryan told reporters in Geneva.\n",
        "\n",
        "“We are breaking the bodies and the minds of the children of Gaza. We are starving the children of Gaza, because if we don’t do something about it we are complicit in what is happening.”\n",
        "\n",
        "Israel strictly controls all inflows of international aid vital for the 2.4 million Palestinians in the Gaza Strip.\n",
        "\n",
        "It halted aid deliveries to Gaza on March 2, days before the collapse of a ceasefire that had significantly reduced hostilities after 15 months of war.\n",
        "\n",
        "Since the start of the blockade, the UN has repeatedly warned of the humanitarian catastrophe on the ground, with famine again looming.\n",
        "\n",
        "Supplies are dwindling and the UN’s World Food Programme (WFP) last Friday said it had sent out its “last remaining food stocks” to kitchens.\n",
        "\n",
        "Ryan pointed to the more than 1,000 children in Gaza that are missing limbs, “thousands of children with spinal cord injuries, with severe head injuries from which they’ll never recover” and psychological conditions.\n",
        "\n",
        "\n",
        " \"\"\"\n",
        "# Generate and print the summary\n",
        "summary = generate_summary(article)\n",
        "print(\"\\nARTICLE:\")\n",
        "print(article)\n",
        "print(\"\\nGENERATED SUMMARY:\")\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}